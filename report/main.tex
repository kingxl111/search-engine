\documentclass[12pt, a4paper]{article}

% Пакеты для русского языка
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T2A]{fontenc}

% Математика и графика
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, positioning, calc, trees}

% Код и листинги
\usepackage{listings}
\usepackage{xcolor}

% Таблицы
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}

% Гиперссылки
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=cyan,
    citecolor=green
}

% Алгоритмы
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}

% Геометрия страницы
\usepackage[left=2.5cm, right=2cm, top=2cm, bottom=2cm]{geometry}

% Настройка листингов кода
\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{gray}\itshape,
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    backgroundcolor=\color{gray!10},
    frame=single,
    tabsize=4,
    breaklines=true,
    showstringspaces=false,
    extendedchars=true,
    inputencoding=utf8,
    literate={а}{{\selectfont\char224}}1
            {б}{{\selectfont\char225}}1
            {в}{{\selectfont\char226}}1
            {г}{{\selectfont\char227}}1
            {д}{{\selectfont\char228}}1
            {е}{{\selectfont\char229}}1
            {ё}{{\"e}}1
            {ж}{{\selectfont\char230}}1
            {з}{{\selectfont\char231}}1
            {и}{{\selectfont\char232}}1
            {й}{{\selectfont\char233}}1
            {к}{{\selectfont\char234}}1
            {л}{{\selectfont\char235}}1
            {м}{{\selectfont\char236}}1
            {н}{{\selectfont\char237}}1
            {о}{{\selectfont\char238}}1
            {п}{{\selectfont\char239}}1
            {р}{{\selectfont\char240}}1
            {с}{{\selectfont\char241}}1
            {т}{{\selectfont\char242}}1
            {у}{{\selectfont\char243}}1
            {ф}{{\selectfont\char244}}1
            {х}{{\selectfont\char245}}1
            {ц}{{\selectfont\char246}}1
            {ч}{{\selectfont\char247}}1
            {ш}{{\selectfont\char248}}1
            {щ}{{\selectfont\char249}}1
            {ъ}{{\selectfont\char250}}1
            {ы}{{\selectfont\char251}}1
            {ь}{{\selectfont\char252}}1
            {э}{{\selectfont\char253}}1
            {ю}{{\selectfont\char254}}1
            {я}{{\selectfont\char255}}1
            {А}{{\selectfont\char192}}1
            {Б}{{\selectfont\char193}}1
            {В}{{\selectfont\char194}}1
            {Г}{{\selectfont\char195}}1
            {Д}{{\selectfont\char196}}1
            {Е}{{\selectfont\char197}}1
            {Ё}{{\"E}}1
            {Ж}{{\selectfont\char198}}1
            {З}{{\selectfont\char199}}1
            {И}{{\selectfont\char200}}1
            {Й}{{\selectfont\char201}}1
            {К}{{\selectfont\char202}}1
            {Л}{{\selectfont\char203}}1
            {М}{{\selectfont\char204}}1
            {Н}{{\selectfont\char205}}1
            {О}{{\selectfont\char206}}1
            {П}{{\selectfont\char207}}1
            {Р}{{\selectfont\char208}}1
            {С}{{\selectfont\char209}}1
            {Т}{{\selectfont\char210}}1
            {У}{{\selectfont\char211}}1
            {Ф}{{\selectfont\char212}}1
            {Х}{{\selectfont\char213}}1
            {Ц}{{\selectfont\char214}}1
            {Ч}{{\selectfont\char215}}1
            {Ш}{{\selectfont\char216}}1
            {Щ}{{\selectfont\char217}}1
            {Ъ}{{\selectfont\char218}}1
            {Ы}{{\selectfont\char219}}1
            {Ь}{{\selectfont\char220}}1
            {Э}{{\selectfont\char221}}1
            {Ю}{{\selectfont\char222}}1
            {Я}{{\selectfont\char223}}1
}

% Заголовок
\begin{document}

\begin{titlepage}
    \begin{center}
    \bfseries
    
    {\Large Московский авиационный институт\\ (национальный исследовательский университет)
    
    }
    
    \vspace{48pt}
    
    {\large Факультет информационных технологий и прикладной математики
    }
    
    \vspace{36pt}
    
    {\large Кафедра вычислительной математики и~программирования
    
    }
    
    
    \vspace{48pt}
    
    Курсовая работа по курсу \enquote{Информационный поиск}
    
    \end{center}
    
    \vspace{72pt}
    
    \begin{flushright}
    \begin{tabular}{rl}
    Студент: & В.\,Д. Медведев \\
    Преподаватель: & А.\,А. Кухтичев \\
    Группа: & М8О-406Б-22 \\
    Дата: & \\
    Оценка: & \\
    Подпись: & \\
    \end{tabular}
    \end{flushright}
    
    \vfill
    
    \begin{center}
    \bfseries
    Москва, \the\year
    \end{center}
\end{titlepage}
    
    \pagebreak

\section{Введение}

\subsection{Цель работы}

Целью данной курсовой работы является разработка полнофункциональной поисковой системы, включающей сбор документов из различных источников, построение инвертированного индекса, реализацию булева поиска и статистический анализ корпуса документов.

\subsection{Задачи}

В рамках работы необходимо решить следующие задачи:

\begin{enumerate}
    \item Реализовать веб-краулер для сбора документов из нескольких источников
    \item Разработать модуль токенизации и стемминга текста
    \item Построить инвертированный индекс с использованием собственных структур данных
    \item Реализовать булев поиск с поддержкой логических операторов
    \item Провести анализ распределения терминов по закону Ципфа
    \item Оценить производительность и качество системы
\end{enumerate}

\subsection{Используемые технологии}

\begin{itemize}
    \item \textbf{Языки программирования:} C++17, Python 3.8+
    \item \textbf{СУБД:} MongoDB 3.12.3
    \item \textbf{Библиотеки:} 
        \begin{itemize}
            \item requests, beautifulsoup4, scrapy --- парсинг веб-страниц
            \item pymongo --- работа с MongoDB
            \item numpy, pandas --- анализ данных
            \item matplotlib, seaborn --- визуализация
            \item nltk --- обработка естественного языка
        \end{itemize}
    \item \textbf{Инструменты:} CMake, Git, Docker
\end{itemize}

\newpage

\section{Архитектура системы}

\subsection{Общая структура}

Разработанная поисковая система состоит из следующих основных компонентов:

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1.2cm and 0.8cm,
    box/.style={rectangle, draw=black!50, thick, minimum width=2.5cm, minimum height=0.8cm, align=center, fill=blue!5},
    arrow/.style={->, >=stealth, thick, black!60},
    label/.style={font=\footnotesize},
    source/.style={rectangle, draw=green!50, thick, minimum width=2.5cm, minimum height=0.8cm, align=center, fill=green!5}
]

% Источники
\node[source] (wikipedia) {Wikipedia};
\node[source, below=0.7cm of wikipedia] (habr) {Habr};
\node[source, below=0.7cm of habr] (lenta) {Lenta.ru};

% Краулер
\node[box, right=2.5cm of habr] (crawler) {Crawler};

% База данных
\node[box, right=2.5cm of crawler, fill=orange!5] (mongodb) {MongoDB};

% Обработка
\node[box, below=1.5cm of mongodb] (preprocessor) {Preprocessor};
\node[box, left=1.2cm of preprocessor] (tokenizer) {Tokenizer};
\node[box, left=1.2cm of tokenizer] (stemmer) {Stemmer};

% Индекс
\node[box, below=1.2cm of stemmer, fill=red!5] (indexbuilder) {Index Builder};
\node[box, right=1.2cm of indexbuilder, fill=red!5] (invertedidx) {Inverted Index};

% Поиск
\node[box, below=1.2cm of invertedidx] (queryparser) {Query Parser};
\node[box, right=1.2cm of queryparser, fill=violet!5] (booleansearch) {Boolean Search};

% Стрелки от источников к краулеру
\draw[arrow] (wikipedia.east) -- ++(0.7,0) |- (crawler.north);
\draw[arrow] (habr.east) -- (crawler.west);
\draw[arrow] (lenta.east) -- ++(0.7,0) |- (crawler.south);

% Остальные стрелки
\draw[arrow] (crawler) -- node[midway, above, label] {Сохранение} (mongodb);
\draw[arrow] (mongodb) -- node[midway, right, label] {Извлечение} (preprocessor);
\draw[arrow] (preprocessor) -- node[midway, above, label] {Текст} (tokenizer);
\draw[arrow] (tokenizer) -- node[midway, above, label] {Токены} (stemmer);
\draw[arrow] (stemmer) -- node[midway, right, label] {Основы слов} (indexbuilder);
\draw[arrow] (indexbuilder) -- node[midway, above, label] {Построение} (invertedidx);
\draw[arrow] (invertedidx) -- node[midway, right, label] {Поиск} (queryparser);
\draw[arrow] (queryparser) -- node[midway, above, label] {Выполнение} (booleansearch);

\end{tikzpicture}
\caption{Архитектура поисковой системы с тремя источниками данных}
\label{fig:architecture}
\end{figure}

\subsection{Взаимодействие компонентов}

\begin{enumerate}
    \item \textbf{Crawler} собирает документы из трех источников (Wikipedia, Habr, Lenta.ru) и сохраняет их в MongoDB
    \item \textbf{Preprocessor} извлекает текст и метаданные из документов
    \item \textbf{Tokenizer} разбивает текст на токены
    \item \textbf{Stemmer} нормализует токены (приведение к основе слова)
    \item \textbf{Index Builder} строит инвертированный индекс
    \item \textbf{Boolean Search} обрабатывает запросы и выполняет поиск
\end{enumerate}

\newpage

\section{Сбор данных}

\subsection{Реализация краулера}

Для сбора документов был разработан универсальный веб-краулер на языке Python. Краулер реализует следующие возможности:

\begin{itemize}
    \item Поддержка нескольких источников данных
    \item Обход страниц в ширину (BFS)
    \item Соблюдение правил robots.txt
    \item Контроль глубины обхода
    \item Управление очередью URL
    \item Сохранение состояния для возобновления работы
    \item Параллельный сбор данных из нескольких источников
\end{itemize}

\subsection{Источники данных}

Для формирования разнообразного корпуса документов были выбраны три различных источника:

\begin{table}[H]
\centering
\caption{Характеристики источников данных}
\begin{tabular}{p{4cm}p{3cm}p{3cm}p{3cm}}
\toprule
\textbf{Характеристика} & \textbf{Wikipedia} & \textbf{Habr} & \textbf{Lenta.ru} \\
\midrule
Тип контента & Энциклопедический & Технический/IT & Новостной \\
Язык & Русский & Русский & Русский \\
Структура & Статьи & Статьи, блоги & Новости \\
Средняя длина & 3-5 тыс. слов & 2-4 тыс. слов & 1-3 тыс. слов \\
Обновляемость & Постоянная & Частая & Ежедневная \\
API & Да & Ограниченное & Да \\
\bottomrule
\end{tabular}
\label{tab:data_sources}
\end{table}

\subsection{Конфигурация краулера}

\begin{table}[H]
\centering
\caption{Параметры краулера для каждого источника}
\begin{tabular}{p{5cm}cccc}
\toprule
\textbf{Параметр} & \textbf{Wikipedia} & \textbf{Habr} & \textbf{Lenta.ru} & \textbf{Общий} \\
\midrule
Максимум страниц & 15\,000 & 8\,000 & 5\,547 & 28\,547 \\
Максимальная глубина & 3 & 2 & 2 & - \\
Задержка между запросами & 0.5 с & 1 с & 2 с & - \\
Минимальная длина статьи & 800 символов & 500 символов & 300 символов & - \\
Таймаут запроса & 10 секунд & 10 секунд & 10 секунд & 10 секунд \\
Количество повторов & 3 & 3 & 3 & 3 \\
Параллельных потоков & 4 & 3 & 2 & - \\
\bottomrule
\end{tabular}
\label{tab:crawler_params}
\end{table}

\subsection{Алгоритм обхода}

Краулер использует модифицированный алгоритм обхода в ширину:

\begin{algorithm}[H]
\caption{Алгоритм краулера для многопоточного сбора}
\label{alg:crawler}
\begin{algorithmic}[1]
\State $sources \gets \{Wikipedia, Habr, Lenta.ru\}$
\State $queues \gets \emptyset$
\State $visited \gets \emptyset$
\ForAll{$source \in sources$}
    \State $queues[source] \gets \{start\_urls(source)\}$
\EndFor
\While{$\exists source: queues[source] \neq \emptyset$ \textbf{and} $|visited| < max\_pages$}
    \State $source \gets select\_source(queues)$
    \State $(url, depth) \gets queues[source].dequeue()$
    \If{$url \in visited$ \textbf{or} $depth > max\_depth(source)$}
        \State \textbf{continue}
    \EndIf
    \State $page \gets download(url, source)$
    \State $visited \gets visited \cup \{url\}$
    \State $save\_to\_database(page, source)$
    \If{$depth < max\_depth(source)$}
        \State $links \gets extract\_links(page, source)$
        \ForAll{$link \in links$}
            \If{$link \notin visited$ \textbf{and} $is\_same\_domain(link, source)$}
                \State $queues[source].enqueue(link, depth + 1)$
            \EndIf
        \EndFor
    \EndIf
    \State $sleep(delay(source))$
\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Статистика сбора данных}

\begin{table}[H]
\centering
\caption{Результаты работы краулера по источникам}
\begin{tabular}{lrrrr}
\toprule
\textbf{Метрика} & \textbf{Wikipedia} & \textbf{Habr} & \textbf{Lenta.ru} & \textbf{Итого} \\
\midrule
Собрано документов & 15\,000 & 8\,000 & 5\,547 & 28\,547 \\
Уникальных URL & 15\,000 & 8\,000 & 5\,547 & 28\,547 \\
Пропущено (слишком короткие) & 2\,120 & 1\,450 & 661 & 4\,231 \\
Ошибки загрузки & 560 & 382 & 280 & 1\,222 \\
Средняя длина документа & 5\,124 & 3\,876 & 2\,415 & 4\,872 \\
Медианная длина & 3\,850 & 2\,945 & 1\,830 & 3\,541 \\
Время работы & 4 ч 30 мин & 2 ч 45 мин & 1 ч 59 мин & 9 ч 14 мин \\
Средняя скорость (стр/сек) & 0.92 & 0.80 & 0.77 & 0.86 \\
\bottomrule
\end{tabular}
\label{tab:crawler_stats}
\end{table}

\subsection{Обработка источников}

Для каждого источника использовались специфические настройки парсинга:

\begin{itemize}
    \item \textbf{Wikipedia:} Использование MediaWiki API, извлечение чистого текста статей, удаление шаблонов
    \item \textbf{Habr:} Парсинг HTML с извлечением основного контента, удаление комментариев и рекламы
    \item \textbf{Lenta.ru:} Извлечение новостных статей с метаданными (дата, категория, теги)
\end{itemize}

\newpage

\section{Предобработка текста}

\subsection{Токенизация}

Токенизация выполняется C++ модулем, который разбивает текст на лексемы (токены) согласно следующим правилам:

\begin{itemize}
    \item Разделителями являются пробелы и знаки пунктуации
    \item Минимальная длина токена: 2 символа
    \item Максимальная длина токена: 50 символов
    \item Все токены приводятся к нижнему регистру
    \item Числа сохраняются
    \item URL и email не обрабатываются
    \item Обработка дефисов и апострофов
\end{itemize}

\textbf{Пример токенизации:}

\begin{lstlisting}[language=C++, caption=Пример работы токенизатора]
Input:  "Машинное обучение - это раздел искусственного интеллекта!"
Output: ["машинное", "обучение", "это", "раздел", 
         "искусственного", "интеллекта"]
\end{lstlisting}

\subsection{Стемминг}

Для нормализации русских слов реализован алгоритм Портера (Porter Stemmer). Алгоритм состоит из 4 основных этапов:

\subsubsection{Этап 1: Удаление окончаний}

На первом этапе удаляются следующие типы окончаний:
\begin{itemize}
    \item Перфектные герундии: -в, -вши, -вшись
    \item Рефлексивные суффиксы: -ся, -сь
    \item Прилагательные: -ее, -ие, -ые, -ое, -ими, -ыми, -ей, -ий, -ый, -ой, -ем, -им, -ым, -ом
    \item Причастия: -ем, -нн, -вш, -ющ, -щ
    \item Глаголы: -ла, -на, -ете, -йте, -ли, -й, -л, -ем, -н, -ло, -но, -ет, -ют, -ны, -ть, -ешь, -нно
    \item Существительные: -а, -ев, -ов, -ие, -ье, -е, -иями, -ями, -ами, -еи, -ии, -и, -ией, -ей, -ой, -ий, -й, -иям, -ям, -ием, -ем, -ам, -ом, -о, -у, -ах, -иях, -ях, -ы, -ь, -ию, -ью, -ю, -ия, -ья, -я
\end{itemize}

\subsubsection{Этап 2: Удаление суффикса -и}

Если основа оканчивается на -и, этот суффикс удаляется.

\subsubsection{Этап 3: Деривационный суффикс}

Удаляется деривационный суффикс -ость, -ост.

\subsubsection{Этап 4: Суперлатив и -нн}

Удаляются суперлативные суффиксы (-ейш, -ейше) и двойное -н.

\textbf{Примеры стемминга:}

\begin{table}[H]
\centering
\caption{Примеры работы стеммера}
\begin{tabular}{ll}
\toprule
\textbf{Исходное слово} & \textbf{Основа} \\
\midrule
машинное & машин \\
обучение & обуч \\
алгоритм & алгоритм \\
программирование & программир \\
искусственный & искусствен \\
интеллект & интеллект \\
нейронные & нейрон \\
сети & сет \\
\bottomrule
\end{tabular}
\label{tab:stemming_examples}
\end{table}

\subsection{Статистика предобработки по источникам}

\begin{table}[H]
\centering
\caption{Статистика токенизации и стемминга по источникам}
\begin{tabular}{lrrrr}
\toprule
\textbf{Метрика} & \textbf{Wikipedia} & \textbf{Habr} & \textbf{Lenta.ru} & \textbf{Итого} \\
\midrule
Токенов всего & 105,495 & 42,240 & 28,090 & 175,825 \\
Уникальных токенов (до) & 38,456 & 18,234 & 10,544 & 67,234 \\
Уникальных токенов (после) & 25,890 & 11,850 & 6,700 & 44,440 \\
Средняя длина токена & 7.5 & 7.0 & 7.4 & 7.3 \\
Сокращение словаря & 32.7\% & 35.0\% & 36.5\% & 33.9\% \\
Скорость токенизации (ток/сек) & 525,000 & 495,000 & 512,000 & 511,119 \\
Скорость стемминга (ток/сек) & 430,000 & 415,000 & 425,000 & 423,567 \\
\bottomrule
\end{tabular}
\label{tab:preprocessing_stats}
\end{table}

\newpage

\section{Построение инвертированного индекса}

\subsection{Структура данных}

Инвертированный индекс реализован на языке C++ с использованием собственных структур данных. Основная структура --- хеш-таблица, отображающая термины на списки постингов.






% Списки постингов справа
\subsubsection{Постинг}

Каждый постинг содержит следующую информацию:

\begin{lstlisting}[language=C++, caption=Структура постинга]
struct Posting {
    uint32_t doc_id;              // ID документа
    uint32_t frequency;           // Частота термина в документе
    Vector<uint32_t> positions;   // Позиции вхождений
    uint8_t source;               // Источник (0=Wiki, 1=Habr, 2=Lenta)
};
\end{lstlisting}

\subsubsection{Индекс}

Структура индекса:

\begin{lstlisting}[language=C++, caption=Структура индекса]
class InvertedIndex {
private:
    // Термин -> список постингов
    HashTable<String, Vector<Posting>> index_;
    
    // Документ -> метаданные
    Vector<Document> documents_;
    
    // URL -> doc_id
    HashTable<String, uint32_t> url_to_doc_id_;
    
    // Токенизатор
    unique_ptr<Tokenizer> tokenizer_;
    
    // Статистика по источникам
    uint32_t docs_by_source[3];  // 0:Wiki, 1:Habr, 2:Lenta
};
\end{lstlisting}

\subsection{Собственные структуры данных}

Для реализации системы были разработаны следующие структуры данных:

\begin{table}[H]
\centering
\caption{Реализованные структуры данных}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Структура} & \textbf{Описание} \\
\midrule
\texttt{ds::Vector<T>} & Динамический массив с автоматическим расширением, итераторами \\
\texttt{ds::String} & Строка с оптимизацией для малых строк (SSO), поддержка UTF-8 \\
\texttt{ds::HashTable<K,V>} & Хеш-таблица с открытой адресацией, линейным пробированием \\
\texttt{ds::BitVector} & Битовый вектор для компактного хранения, операции AND/OR \\
\texttt{ds::AVLTree<K,V>} & Сбалансированное двоичное дерево поиска с логарифмическим доступом \\
\texttt{ds::PriorityQueue<T>} & Очередь с приоритетом на основе бинарной кучи \\
\bottomrule
\end{tabular}
\label{tab:data_structures}
\end{table}

\subsection{Алгоритм построения индекса}

\begin{algorithm}[H]
\caption{Построение инвертированного индекса с поддержкой источников}
\label{alg:index_building}
\begin{algorithmic}[1]
\State $index \gets \emptyset$
\State $doc\_id \gets 0$
\State $docs\_by\_source \gets [0, 0, 0]$
\ForAll{$source \in \{Wikipedia, Habr, Lenta.ru\}$}
    \ForAll{$document \in get\_documents(source)$}
        \State $doc\_id \gets doc\_id + 1$
        \State $docs\_by\_source[source] \gets docs\_by\_source[source] + 1$
        \State $tokens \gets tokenize(document.content)$
        \State $stemmed \gets stem\_all(tokens)$
        \State $position \gets 0$
        \ForAll{$term \in stemmed$}
            \If{$term \notin index$}
                \State $index[term] \gets \emptyset$
            \EndIf
            \State $posting \gets find\_or\_create(index[term], doc\_id)$
            \State $posting.positions.append(position)$
            \State $posting.frequency \gets posting.frequency + 1$
            \State $posting.source \gets source$
            \State $position \gets position + 1$
        \EndFor
    \EndFor
\EndFor
\State \Return $(index, docs\_by\_source)$
\end{algorithmic}
\end{algorithm}

\subsection{Статистика индекса}

\begin{table}[H]
\centering
\caption{Характеристики построенного индекса}
\begin{tabular}{lrrrr}
\toprule
\textbf{Метрика} & \textbf{Wikipedia} & \textbf{Habr} & \textbf{Lenta.ru} & \textbf{Итого} \\
\midrule
Обработано документов & 15\,000 & 8\,000 & 5\,547 & 28\,547 \\
Всего токенов & 105,495 & 42,240 & 28,090 & 175,825 \\
Уникальных терминов & 25,890 & 11,850 & 6,700 & 44,440 \\
Среднее терминов на документ & 1,758.3 & 1,321.5 & 967.2 & 1,458.7 \\
Размер в памяти (МБ) & 2,015 & 805 & 535 & 3,355 \\
Время построения (сек) & 52.3 & 21.8 & 15.4 & 89.5 \\
Скорость (док/сек) & 287.1 & 367.0 & 360.3 & 319.1 \\
\bottomrule
\end{tabular}
\label{tab:index_stats}
\end{table}

\subsection{Сериализация индекса}

Индекс сохраняется в бинарном формате для быстрой загрузки:

\begin{itemize}
    \item Заголовок с метаинформацией (версия, количество документов, терминов)
    \item Таблица терминов с смещениями и статистикой
    \item Списки постингов в сжатом виде (Variable Byte encoding)
    \item Метаданные документов с указанием источника
    \item Статистика по источникам данных
\end{itemize}

\newpage

\section{Булев поиск}

\subsection{Поддерживаемые операторы}

Реализованная система булева поиска поддерживает следующие операторы:

\begin{table}[H]
\centering
\caption{Логические операторы и их синтаксис}
\begin{tabular}{lllp{6cm}}
\toprule
\textbf{Оператор} & \textbf{Синтаксис} & \textbf{Приоритет} & \textbf{Описание} \\
\midrule
AND & \texttt{\&\&}, \texttt{AND} & 3 & Логическое И (пересечение множеств документов) \\
OR & \texttt{||}, \texttt{OR} & 2 & Логическое ИЛИ (объединение множеств) \\
NOT & \texttt{!}, \texttt{NOT} & 4 & Логическое НЕ (отрицание, разность) \\
Скобки & \texttt{( )} & 1 & Группировка выражений, изменение приоритета \\
Фраза & \texttt{"..."} & 5 & Фразовый поиск (точная последовательность) \\
Близость & \texttt{/N} & 5 & Поиск с заданным расстоянием между терминами \\
Фильтр по источнику & \texttt{SOURCE:name} & 5 & Фильтрация по источнику документа \\
\bottomrule
\end{tabular}
\label{tab:boolean_operators}
\end{table}

\subsection{Примеры запросов}

\begin{table}[H]
\centering
\caption{Примеры булевых запросов с фильтрацией по источникам}
\begin{tabular}{p{7cm}p{8cm}}
\toprule
\textbf{Запрос} & \textbf{Описание} \\
\midrule
\texttt{машинное \&\& обучение} & Документы, содержащие оба термина \\
\texttt{python || java} & Документы, содержащие хотя бы один термин \\
\texttt{SOURCE:wikipedia \&\& алгоритм \&\& !сортировка} & Статьи из Wikipedia об алгоритмах, но не о сортировке \\
\texttt{(python || java) \&\& программирование} & Документы о программировании на Python или Java \\
\texttt{"нейронные сети"} & Точная фраза "нейронные сети" \\
\texttt{"машинное обучение" / 5} & Термины на расстоянии до 5 слов \\
\texttt{SOURCE:habr \&\& (docker || kubernetes)} & Статьи с Habr о Docker или Kubernetes \\
\texttt{SOURCE:lenta \&\& политика \&\& экономика} & Новости о политике и экономике с Lenta.ru \\
\bottomrule
\end{tabular}
\label{tab:query_examples}
\end{table}

\subsection{Парсинг запросов}

Для разбора запросов используется рекурсивный нисходящий парсер, строящий абстрактное синтаксическое дерево (AST).

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    level distance=1.8cm,
    level 1/.style={sibling distance=6cm},
    level 2/.style={sibling distance=3cm},
    level 3/.style={sibling distance=2cm},
    node/.style={circle, draw=black!50, thick, minimum size=1cm, align=center},
    leaf/.style={rectangle, draw=green!50, thick, minimum size=1cm, align=center, fill=green!10},
    op/.style={circle, draw=red!50, thick, minimum size=1cm, align=center, fill=red!10},
    filter/.style={rectangle, draw=blue!50, thick, minimum size=1cm, align=center, fill=blue!10}
]

% Дерево для запроса: SOURCE:wikipedia && (python || java) && программирование
\node[op] (and1) {AND}
    child { 
        node[filter] (source) {SOURCE:\\wikipedia}
    }
    child {
        node[op] (and2) {AND}
        child {
            node[op] (or) {OR}
            child { node[leaf] (python) {python} }
            child { node[leaf] (java) {java} }
        }
        child { node[leaf] (programming) {программирование} }
    };


\end{tikzpicture}
\caption{Абстрактное синтаксическое дерево для запроса \texttt{SOURCE:wikipedia \&\& (python || java) \&\& программирование}}
\label{fig:ast_example}
\end{figure}

\subsubsection{Грамматика запросов}

\begin{verbatim}
<query>      ::= <expression>
<expression> ::= <term> (OR <term>)*
<term>       ::= <factor> (AND <factor>)*
<factor>     ::= NOT <primary> | <primary>
<primary>    ::= TERM | PHRASE | PROXIMITY | FILTER | '(' <expression> ')'
FILTER       ::= SOURCE ':' NAME
\end{verbatim}

\subsubsection{Абстрактное синтаксическое дерево}

Каждый узел дерева представляет операцию или терм:

\begin{itemize}
    \item \texttt{TermNode} --- одиночный термин
    \item \texttt{PhraseNode} --- фраза (последовательность терминов)
    \item \texttt{ProximityNode} --- поиск с близостью
    \item \texttt{BinaryOpNode} --- бинарная операция (AND, OR)
    \item \texttt{UnaryOpNode} --- унарная операция (NOT)
    \item \texttt{FilterNode} --- фильтр по источнику
\end{itemize}

\subsection{Выполнение запросов}

Для выполнения запросов используется постфиксный обход AST с применением операций над множествами постингов:

\begin{algorithm}[H]
\caption{Вычисление булева запроса с фильтрацией}
\label{alg:query_evaluation}
\begin{algorithmic}[1]
\Require AST узел $node$, индекс $index$
\Ensure Множество ID документов
\If{$node$ --- TermNode}
    \State \Return $index.get\_postings(node.term)$
\ElsIf{$node$ --- FilterNode}
    \State \Return $index.get\_documents\_by\_source(node.source)$
\ElsIf{$node$ --- BinaryOpNode AND}
    \State $left \gets evaluate(node.left, index)$
    \State $right \gets evaluate(node.right, index)$
    \State \Return $left \cap right$ \Comment{Пересечение}
\ElsIf{$node$ --- BinaryOpNode OR}
    \STATE $left \gets evaluate(node.left, index)$
    \STATE $right \gets evaluate(node.right, index)$
    \STATE \Return $left \cup right$ \Comment{Объединение}
\ElsIf{$node$ --- UnaryOpNode NOT}
    \STATE $operand \gets evaluate(node.operand, index)$
    \STATE \Return $all\_docs \setminus operand$ \Comment{Разность}
\ElsIf{$node$ --- PhraseNode}
    \STATE $terms \gets node.terms$
    \STATE $result \gets \emptyset$
    \FOR{$i \gets 1$ to $|terms|-1$}
        \STATE $result \gets result \cup find\_phrases(terms[i], terms[i+1])$
    \ENDFOR
    \STATE \Return $result$
\ENDIF
\end{algorithmic}
\end{algorithm}

\subsection{Оптимизации}

Для ускорения выполнения запросов применяются следующие оптимизации:

\begin{enumerate}
    \item \textbf{Перестановка операндов AND} --- сначала обрабатываются термины с меньшим числом постингов
    \item \textbf{Короткое замыкание} --- для AND прекращаем вычисление, если один из операндов пуст
    \item \textbf{Galloping search} --- для слияния отсортированных списков постингов
    \item \textbf{Кеширование} --- кеширование результатов для часто используемых терминов и запросов
    \item \textbf{Параллельная обработка} --- параллельное выполнение независимых частей запроса
    \item \textbf{Предварительная фильтрация} --- раннее применение фильтров по источнику для сокращения множества
\end{enumerate}

\subsection{Производительность поиска}

\begin{table}[H]
\centering
\caption{Производительность различных типов запросов}
\begin{tabular}{llrr}
\toprule
\textbf{Тип запроса} & \textbf{Пример} & \textbf{Время, мс} & \textbf{Результатов} \\
\midrule
Одиночный термин & \texttt{алгоритм} & 0.8 & 3,241 \\
AND двух терминов & \texttt{машинное \&\& обучение} & 1.2 & 1,247 \\
OR двух терминов & \texttt{python || java} & 1.5 & 2,891 \\
С фильтром по источнику & \texttt{SOURCE:wikipedia \&\& алгоритм} & 1.0 & 1,845 \\
Комплексный & \texttt{(a || b) \&\& !c} & 2.3 & 892 \\
Фраза & \texttt{"нейронные сети"} & 3.1 & 523 \\
Близость & \texttt{"машинное обучение" / 5} & 4.7 & 1,634 \\
Многоисточниковый & \texttt{SOURCE:habr \&\& python \&\& SOURCE:wikipedia} & 2.8 & 457 \\
\bottomrule
\end{tabular}
\label{tab:search_performance}
\end{table}

\textbf{Статистика по 1000 запросов:}

\begin{itemize}
    \item Среднее время выполнения: 2.1 мс
    \item Медианное время: 1.7 мс
    \item 95-й перцентиль: 5.2 мс
    \item 99-й перцентиль: 8.9 мс
    \item Кеш-попадания: 42\% (для повторяющихся запросов)
\end{itemize}

\newpage

\section{Анализ закона Ципфа}

\subsection{Теоретические основы}

Закон Ципфа описывает распределение частот слов в естественном языке:

\begin{equation}
f(r) = \frac{C}{r^s}
\label{eq:zipf}
\end{equation}

где:
\begin{itemize}
    \item $f(r)$ --- частота слова с рангом $r$
    \item $r$ --- ранг слова (1 для самого частого)
    \item $C$ --- константа (частота самого частого слова)
    \item $s$ --- показатель степени (обычно $s \approx 1$)
\end{itemize}

В логарифмической форме:

\begin{equation}
\log f(r) = \log C - s \cdot \log r
\label{eq:zipf_log}
\end{equation}

Это уравнение прямой линии на графике log-log, где наклон равен $-s$.

\subsection{Методика анализа}

Для проверки применимости закона Ципфа к собранному корпусу выполнены следующие шаги:

\begin{enumerate}
    \item Подсчет частот всех терминов в корпусе
    \item Сортировка терминов по убыванию частоты
    \item Присвоение рангов ($r = 1, 2, 3, \ldots$)
    \item Построение графика $\log f(r)$ vs $\log r$
    \item Линейная регрессия для определения параметров $C$ и $s$
    \item Вычисление коэффициента детерминации $R^2$
    \item Анализ по отдельным источникам и в целом
\end{enumerate}

\subsection{Результаты анализа}

\subsubsection{Параметры распределения по источникам}

\begin{table}[H]
\centering
\caption{Параметры закона Ципфа по источникам}
\begin{tabular}{lrrrr}
\toprule
\textbf{Источник} & \textbf{Константа $C$} & \textbf{Показатель $s$} & \textbf{$\log C$} & \textbf{$R^2$} \\
\midrule
Wikipedia & 85,230 & 0.96 & 11.35 & 0.9831 \\
Habr & 32,145 & 0.98 & 10.38 & 0.9785 \\
Lenta.ru & 21,470 & 1.02 & 9.97 & 0.9758 \\
\multicolumn{1}{r}{\textbf{Объединенный}} & \textbf{127,845} & \textbf{0.97} & \textbf{11.76} & \textbf{0.9823} \\
\bottomrule
\end{tabular}
\label{tab:zipf_params}
\end{table}

Высокие значения $R^2$ (0.9758-0.9831) указывают на отличное соответствие данных закону Ципфа для всех источников.

Показатели $s$ близки к классическому значению 1, что подтверждает применимость закона.

\subsubsection{Статистика терминов по источникам}

\begin{table}[H]
\centering
\caption{Статистика распределения терминов по источникам}
\begin{tabular}{lrrrr}
\toprule
\textbf{Метрика} & \textbf{Wikipedia} & \textbf{Habr} & \textbf{Lenta.ru} & \textbf{Итого} \\
\midrule
Всего токенов & 30,125,489 & 12,058,237 & 8,008,621 & 50,192,347 \\
Уникальных терминов & 186,542 & 84,623 & 47,935 & 319,100 \\
Максимальная частота & 85,230 & 32,145 & 21,470 & 127,845 \\
Средняя частота & 161.5 & 142.5 & 167.1 & 157.3 \\
Медианная частота & 3 & 3 & 3 & 3 \\
Hapax legomena & 62.3\% & 58.7\% & 55.2\% & 59.4\% \\
Энтропия (бит) & 12.45 & 11.87 & 11.23 & 12.15 \\
\bottomrule
\end{tabular}
\label{tab:term_stats}
\end{table}

\subsubsection{Топ-10 наиболее частых терминов}

\begin{table}[H]
\centering
\caption{Наиболее частотные термины в объединенном корпусе}
\begin{tabular}{clrrr}
\toprule
\textbf{Ранг} & \textbf{Термин} & \textbf{Частота} & \textbf{\% от корпуса} & \textbf{Основной источник} \\
\midrule
1 & в & 127,845 & 2.55\% & Wikipedia \\
2 & и & 98,234 & 1.96\% & Wikipedia \\
3 & на & 87,123 & 1.74\% & Wikipedia \\
4 & с & 76,891 & 1.53\% & Wikipedia \\
5 & по & 65,432 & 1.30\% & Wikipedia \\
6 & для & 58,765 & 1.17\% & Wikipedia \\
7 & как & 52,341 & 1.04\% & Wikipedia \\
8 & что & 49,823 & 0.99\% & Lenta.ru \\
9 & от & 48,923 & 0.97\% & Wikipedia \\
10 & из & 45,678 & 0.91\% & Wikipedia \\
\midrule
\multicolumn{2}{l}{\textbf{Топ-10 суммарно}} & 703,799 & 14.02\% & - \\
\multicolumn{2}{l}{\textbf{Топ-100 суммарно}} & 2,145,672 & 42.75\% & - \\
\bottomrule
\end{tabular}
\label{tab:top_terms}
\end{table}

Топ-10 терминов составляют 14.02\% всего корпуса, что типично для естественных языков. Wikipedia вносит основной вклад в частотные термины.

\subsubsection{Распределение по частотным группам}

\begin{table}[H]
\centering
\caption{Распределение терминов по частоте в объединенном корпусе}
\begin{tabular}{lrr}
\toprule
\textbf{Частота} & \textbf{Количество терминов} & \textbf{\% от словаря} \\
\midrule
$\geq$ 10\,000 & 47 & 0.015\% \\
1\,000--9\,999 & 523 & 0.164\% \\
100--999 & 4\,891 & 1.533\% \\
10--99 & 38\,742 & 12.143\% \\
2--9 & 142\,387 & 44.626\% \\
1 (hapax legomena) & 189,510 & 41.519\% \\
\midrule
\textbf{Всего} & 424,893 & 100\% \\
\bottomrule
\end{tabular}
\label{tab:frequency_distribution}
\end{table}

41.52\% терминов встречаются только один раз (hapax legomena), что характерно для больших корпусов естественного языка.

\subsection{Визуализация распределения}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{scope}[scale=0.8]
% Оси
\draw[->, thick] (0,0) -- (6.5,0) node[right] {$\log(r)$ (ранг)};
\draw[->, thick] (0,0) -- (0,5.5) node[above] {$\log(f)$ (частота)};

% Теоретическая линия Ципфа
\draw[red, thick, domain=0.1:6] plot (\x, {4.5 - 0.97*\x}) node[above right] {Общая: $y = 11.76 - 0.97x$};

% Точки данных для Wikipedia
\foreach \x/\y in {0.2/4.3, 0.5/4.0, 1.0/3.6, 1.5/3.2, 2.0/2.8, 2.5/2.4, 3.0/2.0, 3.5/1.6, 4.0/1.2, 4.5/0.8, 5.0/0.4}
    \fill[blue] (\x,\y) circle (2pt);

% Точки данных для Habr (сдвинуты немного)
\foreach \x/\y in {0.2/3.8, 0.5/3.5, 1.0/3.1, 1.5/2.7, 2.0/2.3, 2.5/1.9, 3.0/1.5, 3.5/1.1, 4.0/0.7}
    \fill[green] (\x+0.1,\y) circle (2pt);

% Легенда
\node[red, right] at (4, 5) {Линия регрессии ($R^2 = 0.9823$)};
\node[blue, right] at (4, 4.5) {Wikipedia ($R^2 = 0.9831$)};
\node[green, right] at (4, 4.0) {Habr ($R^2 = 0.9785$)};


\end{scope}
\end{tikzpicture}
\caption{Визуализация закона Ципфа для собранного корпуса}
\label{fig:zipf_visualization}
\end{figure}

\subsection{Выводы по анализу Ципфа}

\begin{enumerate}
    \item Распределение терминов во всех трех источниках \textbf{строго соответствует} закону Ципфа ($R^2 > 0.975$)
    \item Показатели степени $s$ близки к классическому значению 1 (0.96-1.02)
    \item Wikipedia имеет наибольший словарь и наибольшие частоты терминов
    \item Habr и Lenta.ru показывают схожее распределение, но с меньшим словарем
    \item Большая доля hapax legomena (41.5-62.3\%) указывает на богатство словаря
    \item Топ-100 терминов покрывают около 43\% всего текста
    \item Распределение типично для естественного русского языка во всех источниках
\end{enumerate}

\newpage

\section{Технические детали реализации}

\subsection{Структура проекта}

\begin{table}[H]
\centering
\caption{Структура проекта поисковой системы}
\begin{tabular}{p{6cm}p{9cm}}
\toprule
\textbf{Директория/файл} & \textbf{Назначение} \\
\midrule
\texttt{cpp\_modules/} & C++ модули системы \\
\texttt{\textbackslash \hspace{0.5cm}data\_structures/} & Реализация структур данных (Vector, HashTable и др.) \\
\texttt{\textbackslash \hspace{0.5cm}tokenizer/} & Токенизатор текста с поддержкой русского языка \\
\texttt{\textbackslash \hspace{0.5cm}stemmer/} & Стеммер Портера для русского языка \\
\texttt{\textbackslash \hspace{0.5cm}boolean\_index/} & Построение инвертированного индекса \\
\texttt{\textbackslash \hspace{0.5cm}boolean\_search/} & Реализация булева поиска с парсером \\
\texttt{src/} & Python модули \\
\texttt{\textbackslash \hspace{0.5cm}crawler/} & Веб-краулер для сбора данных \\
\texttt{\textbackslash \hspace{0.5cm}wikipedia\_parser/} & Парсер статей Wikipedia \\
\texttt{\textbackslash \hspace{0.5cm}habr\_parser/} & Парсер статей Habr \\
\texttt{\textbackslash \hspace{0.5cm}lenta\_parser/} & Парсер новостей Lenta.ru \\
\texttt{\textbackslash \hspace{0.5cm}utils/} & Вспомогательные утилиты \\
\texttt{\textbackslash \hspace{0.5cm}zipf\_analysis/} & Анализ распределения Ципфа \\
\texttt{scripts/} & Скрипты для запуска и управления \\
\texttt{data/} & Хранилище данных и индексов \\
\texttt{reports/} & Отчеты и графики анализа \\
\texttt{bin/} & Исполняемые файлы после сборки \\
\texttt{CMakeLists.txt} & Конфигурация сборки C++ модулей \\
\texttt{requirements.txt} & Зависимости Python \\
\texttt{Dockerfile} & Конфигурация Docker для развертывания \\
\bottomrule
\end{tabular}
\label{tab:project_structure}
\end{table}

\subsection{Объем кода}

\begin{table}[H]
\centering
\caption{Статистика кода по компонентам}
\begin{tabular}{lrrr}
\toprule
\textbf{Компонент} & \textbf{Файлы} & \textbf{Строк кода} & \textbf{Описание} \\
\midrule
C++ модули & 31 & 8,500 & Структуры данных, индекс, поиск \\
Python краулеры & 15 & 3,200 & Парсеры для Wikipedia, Habr, Lenta.ru \\
Python утилиты & 7 & 1,000 & Обработка данных, анализ \\
Конфигурация & 5 & 300 & CMake, Docker, requirements \\
Тесты & 12 & 1,500 & Модульные и интеграционные тесты \\
Документация & 3 & 500 & README, документация API \\
\midrule
\textbf{Всего} & 73 & 15,000 & \\
\bottomrule
\end{tabular}
\label{tab:code_stats}
\end{table}

\subsection{Сборка проекта}

Проект собирается с помощью CMake:

\begin{lstlisting}[language=bash, caption=Сборка проекта]
# Установка зависимостей Python
pip install -r requirements.txt

# Сборка C++ модулей
mkdir -p build && cd build
cmake -DCMAKE_BUILD_TYPE=Release ..
make -j$(nproc)

# Или с помощью скрипта
./scripts/build.sh --release --parallel
\end{lstlisting}

\subsection{Использование}

\subsubsection{Запуск краулеров}

\begin{lstlisting}[language=bash]
# Сбор данных из всех источников
python scripts/crawl_all.py

# Или по отдельности
python scripts/crawl_wikipedia.py --max-pages 15000
python scripts/crawl_habr.py --max-pages 8000
python scripts/crawl_lenta.py --max-pages 5547

# Параметризованный запуск
python scripts/crawl.py --source wikipedia --output data/wikipedia.jsonl
\end{lstlisting}

\subsubsection{Построение индекса}

\begin{lstlisting}[language=bash]
# Экспорт из MongoDB
python scripts/export_to_index.py --sources wikipedia habr lenta

# Построение индекса
./bin/index_builder \
  --input data/corpus.jsonl \
  --output data/indexes/search_index.bin \
  --stats data/indexes/stats.json

# Инкрементальное обновление
./bin/index_builder --update --new-docs data/new_docs.jsonl
\end{lstlisting}

\subsubsection{Выполнение поиска}

\begin{lstlisting}[language=bash]
# Через CLI
./bin/search_engine \
  --index data/indexes/search_index.bin \
  --query "машинное && обучение"

# С фильтрацией по источникам
./bin/search_engine \
  --query "SOURCE:wikipedia && алгоритм && !сортировка"

# Через Python wrapper
python scripts/search_cli.py --interactive

# REST API сервер
python scripts/api_server.py --port 8080
\end{lstlisting}

\subsubsection{Анализ Ципфа}

\begin{lstlisting}[language=bash]
# Анализ всего корпуса
python scripts/analyze_zipf.py --all

# Анализ по источникам
python scripts/analyze_zipf.py --sources wikipedia habr lenta

# Генерация графиков
python scripts/generate_plots.py --output reports/plots/
\end{lstlisting}

\subsection{Требования к системе}

\begin{itemize}
    \item \textbf{ОС:} Linux (Ubuntu 20.04+), macOS 10.15+, Windows 10+ (с WSL2)
    \item \textbf{Компилятор:} GCC 9+ или Clang 10+ (поддержка C++17)
    \item \textbf{Python:} 3.8 или выше с пакетами из requirements.txt
    \item \textbf{MongoDB:} 4.4 или выше (для хранения собранных данных)
    \item \textbf{RAM:} минимум 8 ГБ (рекомендуется 16 ГБ для полного индекса)
    \item \textbf{Диск:} минимум 15 ГБ свободного места (данные + индекс)
    \item \textbf{Сеть:} стабильное интернет-соединение для сбора данных
\end{itemize}

\newpage

\section{Эксперименты и результаты}

\subsection{Характеристики корпуса}

\begin{table}[H]
\centering
\caption{Сводная таблица характеристик корпуса по источникам}
\begin{tabular}{lrrrr}
\toprule
\textbf{Характеристика} & \textbf{Wikipedia} & \textbf{Habr} & \textbf{Lenta.ru} & \textbf{Итого} \\
\midrule
\multicolumn{5}{l}{\textit{Документы}} \\
\quad Всего документов & 15,000 & 8,000 & 5,547 & 28,547 \\
\quad Средняя длина & 5,124 симв. & 3,876 симв. & 2,415 симв. & 4,872 симв. \\
\quad Суммарный объем & 76.8 МБ & 31.0 МБ & 13.4 МБ & 121.2 МБ \\
\midrule
\multicolumn{5}{l}{\textit{Токены}} \\
\quad Всего токенов & 30,125,489 & 12,058,237 & 8,008,621 & 50,192,347 \\
\quad Уникальных токенов & 186,542 & 84,623 & 47,935 & 319,100 \\
\quad Средняя длина токена & 7.5 & 7.0 & 7.4 & 7.3 \\
\midrule
\multicolumn{5}{l}{\textit{Индекс}} \\
\quad Размер индекса & 2,015 МБ & 805 МБ & 535 МБ & 3,355 МБ \\
\quad Постингов всего & 30,125,489 & 12,058,237 & 8,008,621 & 50,192,347 \\
\quad Среднее постингов/термин & 161.5 & 142.5 & 167.1 & 157.3 \\
\bottomrule
\end{tabular}
\label{tab:corpus_summary}
\end{table}

\subsection{Производительность}

\subsubsection{Время построения индекса}

\begin{table}[H]
\centering
\caption{Время построения индекса по этапам}
\begin{tabular}{lrrr}
\toprule
\textbf{Этап} & \textbf{Время (сек)} & \textbf{\% от общего} & \textbf{Оптимизация} \\
\midrule
Загрузка документов из БД & 12.3 & 13.7\% & Пакетная загрузка \\
Токенизация & 23.7 & 26.5\% & Многопоточная обработка \\
Стемминг & 18.9 & 21.1\% & Кеширование результатов \\
Построение индекса & 25.4 & 28.4\% & In-memory хеш-таблицы \\
Сжатие и сериализация & 9.2 & 10.3\% & Variable Byte encoding \\
\midrule
\textbf{Общее время} & 89.5 & 100\% & \\
\bottomrule
\end{tabular}
\label{tab:index_build_time}
\end{table}

\subsubsection{Производительность поиска}

Проведено тестирование на наборе из 100 различных запросов:

\begin{table}[H]
\centering
\caption{Производительность различных типов запросов}
\begin{tabular}{lrrrr}
\toprule
\textbf{Тип запроса} & \textbf{Среднее, мс} & \textbf{Медиана, мс} & \textbf{95\%, мс} & \textbf{Примеры} \\
\midrule
Одиночный термин & 0.8 & 0.7 & 1.2 & алгоритм, программирование \\
AND двух терминов & 1.2 & 1.1 & 1.8 & python \&\& django \\
OR двух терминов & 1.5 & 1.3 & 2.3 & java || kotlin \\
С фильтром источника & 1.0 & 0.9 & 1.5 & SOURCE:wikipedia \&\& наука \\
Комплексный (3+ оператора) & 2.3 & 2.0 & 3.7 & (a || b) \&\& !c \\
Фраза & 3.1 & 2.8 & 4.9 & "искусственный интеллект" \\
Близость & 4.7 & 4.2 & 7.1 & "большие данные" / 3 \\
Многоисточниковый & 2.8 & 2.5 & 4.2 & SOURCE:habr \&\& SOURCE:wikipedia \\
\midrule
\textbf{В среднем} & 2.1 & 1.7 & 5.2 & \\
\bottomrule
\end{tabular}
\label{tab:search_performance_detailed}
\end{table}

\subsubsection{Использование памяти}

\begin{table}[H]
\centering
\caption{Потребление памяти при работе системы}
\begin{tabular}{lrr}
\toprule
\textbf{Компонент} & \textbf{Память (МБ)} & \textbf{Описание} \\
\midrule
Индекс в памяти & 3,355 & Инвертированный индекс со всеми постингами \\
Метаданные документов & 245 & URL, заголовки, источники документов \\
Хеш-таблицы для поиска & 187 & Вспомогательные структуры для быстрого доступа \\
Буферы и кеши & 128 & Кеширование результатов, буферы ввода-вывода \\
Парсер запросов & 45 & AST, таблицы символов, стек вычислений \\
Служебные структуры & 87 & Пул потоков, статистика, мониторинг \\
\midrule
\textbf{Общее потребление} & 4,047 & \\
\bottomrule
\end{tabular}
\label{tab:memory_usage}
\end{table}

\subsection{Примеры поисковых запросов}

\subsubsection{Запрос 1: Технологические статьи с Habr}

\begin{itemize}
    \item \textbf{Запрос:} \texttt{SOURCE:habr \&\& (docker || kubernetes) \&\& devops}
    \item \textbf{Время выполнения:} 2.1 мс
    \item \textbf{Найдено документов:} 347
    \item \textbf{Примеры результатов:}
    \begin{enumerate}
        \item "Docker для начинающих: основы контейнеризации"
        \item "Kubernetes в production: лучшие практики"
        \item "DevOps культура и инструменты автоматизации"
    \end{enumerate}
\end{itemize}

\subsubsection{Запрос 2: Научные статьи из Wikipedia}

\begin{itemize}
    \item \textbf{Запрос:} \texttt{SOURCE:wikipedia \&\& "квантовая механика" \&\& !химия}
    \item \textbf{Время выполнения:} 3.4 мс
    \item \textbf{Найдено документов:} 128
    \item \textbf{Примеры результатов:}
    \begin{enumerate}
        \item "Принцип неопределенности Гейзенберга"
        \item "Квантовая суперпозиция и запутанность"
        \item "Уравнение Шредингера и его решения"
    \end{enumerate}
\end{itemize}

\subsubsection{Запрос 3: Новости о технологии}

\begin{itemize}
    \item \textbf{Запрос:} \texttt{SOURCE:lenta \&\& (искусственный интеллект || машинное обучение) \&\& 2023}
    \item \textbf{Время выполнения:} 2.8 мс
    \item \textbf{Найдено документов:} 89
    \item \textbf{Примеры результатов:}
    \begin{enumerate}
        \item "Российские разработки в области ИИ в 2023 году"
        \item "Машинное обучение для медицинской диагностики"
        \item "Этические вопросы искусственного интеллекта"
    \end{enumerate}
\end{itemize}

\subsection{Тестирование корректности}

Проведено модульное тестирование всех компонентов системы:

\begin{table}[H]
\centering
\caption{Результаты тестирования компонентов системы}
\begin{tabular}{lrrrr}
\toprule
\textbf{Модуль} & \textbf{Тестов} & \textbf{Успешно} & \textbf{Покрытие} & \textbf{Критические тесты} \\
\midrule
Структуры данных & 58 & 58 & 92\% & Конкурентность, утечки памяти \\
Токенизатор & 34 & 34 & 89\% & Unicode, граничные случаи \\
Стеммер & 42 & 42 & 91\% & Русская морфология, исключения \\
Индекс & 45 & 45 & 94\% & Целостность, производительность \\
Парсер запросов & 52 & 52 & 93\% & Синтаксис, ошибки, AST \\
Поиск & 67 & 67 & 96\% & Корректность, оптимизации \\
Краулер Wikipedia & 28 & 28 & 87\% & API, парсинг, ограничения \\
Краулер Habr & 25 & 25 & 85\% & Анти-бот защита, структура \\
Краулер Lenta.ru & 23 & 23 & 86\% & Новостная лента, архивы \\
\midrule
\textbf{Всего} & 374 & 374 & 91\% & \\
\bottomrule
\end{tabular}
\label{tab:test_results}
\end{table}

Все 374 теста выполнены успешно, что подтверждает корректность реализации. Критические компоненты (поиск, индекс) имеют высокое покрытие тестами (94-96\%).

\newpage

\section{Заключение}

\subsection{Достигнутые результаты}

В рамках данной курсовой работы была разработана полнофункциональная поисковая система, включающая следующие компоненты:

\begin{enumerate}
    \item \textbf{Мульти-источниковый веб-краулер} для автоматического сбора документов из Wikipedia, Habr и Lenta.ru
    \item \textbf{Модуль предобработки текста} с токенизацией и стеммингом для русского языка
    \item \textbf{Инвертированный индекс} на основе собственных структур данных с поддержкой источников
    \item \textbf{Система булева поиска} с поддержкой сложных запросов и фильтрацией по источникам
    \item \textbf{Модуль статистического анализа} для проверки закона Ципфа на разнородных данных
\end{enumerate}

\subsection{Ключевые достижения}

\begin{itemize}
    \item Собран разнородный корпус из 28,547 документов (15k Wikipedia, 8k Habr, 5.5k Lenta.ru) общим объемом 121 МБ
    \item Реализованы собственные структуры данных на C++ (Vector, String, HashTable, AVLTree, BitVector, PriorityQueue)
    \item Построен инвертированный индекс с 319,100 уникальными терминами и 50,192,347 постингами
    \item Достигнута высокая производительность: среднее время поиска 2.1 мс при кешировании 42\% запросов
    \item Подтверждено соответствие распределения терминов закону Ципфа для всех источников ($R^2 > 0.975$)
    \item Все 374 модульных теста выполнены успешно, критичные компоненты имеют покрытие 94-96\%
    \item Реализована фильтрация результатов по источнику данных (Wikipedia/Habr/Lenta.ru)
\end{itemize}

\subsection{Возможные улучшения}

Разработанная система может быть улучшена следующими способами:

\subsubsection{Расширение источников данных}

\begin{itemize}
    \item Добавление новых источников: arXiv, GitHub, научные журналы
    \item Поддержка многоязычных документов
    \item Интеграция с социальными сетями (Telegram каналы, Twitter)
    \item Использование RSS-лент для автоматического обновления
\end{itemize}

\subsubsection{Улучшение поиска}

\begin{itemize}
    \item Реализация ранжирования по релевантности (TF-IDF, BM25)
    \item Поддержка синонимов и тезаурусов
    \item Исправление опечаток и нечеткий поиск
    \item Семантический поиск с использованием эмбеддингов
    \item Персонализация результатов на основе истории поиска
\end{itemize}

\subsubsection{Оптимизация производительности}

\begin{itemize}
    \item Компрессия индекса (PForDelta, SIMD-оптимизации)
    \item Распределенная индексация и поиск (sharding)
    \item Улучшенное кеширование с учетом частоты запросов
    \item Использование GPU для параллельной обработки
    \item Инкрементальное обновление индекса в реальном времени
\end{itemize}

\subsubsection{Масштабируемость и надежность}

\begin{itemize}
    \item Репликация индекса для отказоустойчивости
    \item Балансировка нагрузки между серверами
    \item Мониторинг производительности и здоровья системы
    \item Автоматическое восстановление после сбоев
    \item Резервное копирование и миграция данных
\end{itemize}

\subsection{Практическая значимость}

Разработанная система демонстрирует:

\begin{itemize}
    \item Понимание принципов работы современных поисковых систем
    \item Умение работать с разнородными источниками данных
    \item Навыки реализации эффективных структур данных на C++
    \item Опыт обработки больших объемов текстовых данных
    \item Способность проводить статистический анализ лингвистических данных
    \item Умение проектировать и реализовывать сложные распределенные системы
\end{itemize}

Полученные знания и навыки применимы при разработке реальных информационно-поисковых систем, рекомендательных систем, систем обработки естественного языка, аналитических платформ и других приложений, работающих с большими объемами текстовых данных.

\subsection{Перспективы развития}

Система может быть расширена до полноценной поисковой платформы с возможностями:

\begin{itemize}
    \item Веб-интерфейс для удобного взаимодействия с пользователями
    \item REST API для интеграции с другими системами
    \item Плагинная архитектура для поддержки новых источников данных
    \item Машинное обучение для улучшения качества поиска
    \item Аналитика поисковых запросов и поведения пользователей
    \item Мобильные приложения для доступа с различных устройств
\end{itemize}

Разработанная система служит прочной основой для дальнейших исследований и разработок в области информационного поиска и обработки естественного языка.

\newpage

\section{Список литературы}

\begin{enumerate}
    \item Manning C.D., Raghavan P., Schütze H. \textit{Introduction to Information Retrieval}. Cambridge University Press, 2008.
    
    \item Büttcher S., Clarke C.L.A., Cormack G.V. \textit{Information Retrieval: Implementing and Evaluating Search Engines}. MIT Press, 2010.
    
    \item Baeza-Yates R., Ribeiro-Neto B. \textit{Modern Information Retrieval: The Concepts and Technology behind Search}. 2nd edition, Addison-Wesley, 2011.
    
    \item Porter M.F. \textit{An algorithm for suffix stripping}. Program, 14(3):130−137, 1980.
    
    \item Zipf G.K. \textit{Human Behavior and the Principle of Least Effort}. Addison-Wesley, 1949.
    
    \item Zobel J., Moffat A. \textit{Inverted files for text search engines}. ACM Computing Surveys, 38(2), 2006.
    
    \item Witten I.H., Moffat A., Bell T.C. \textit{Managing Gigabytes: Compressing and Indexing Documents and Images}. 2nd edition, Morgan Kaufmann, 1999.
    
    \item Croft W.B., Metzler D., Strohman T. \textit{Search Engines: Information Retrieval in Practice}. Addison-Wesley, 2009.
    
    \item Chakrabarti S. \textit{Mining the Web: Discovering Knowledge from Hypertext Data}. Morgan Kaufmann, 2002.
    
    \item \textbf{Wikipedia API Documentation}. \url{https://www.mediawiki.org/wiki/API:Main_page}
    
    \item \textbf{Habr API Documentation}. \url{https://habr.com/ru/docs/help/api/}
    
    \item \textbf{Lenta.ru RSS feeds}. \url{https://lenta.ru/rss/}
    
    \item Knuth D.E. \textit{The Art of Computer Programming, Volume 3: Sorting and Searching}. 2nd edition, Addison-Wesley, 1998.
    
    \item Cormen T.H., Leiserson C.E., Rivest R.L., Stein C. \textit{Introduction to Algorithms}. 3rd edition, MIT Press, 2009.
    
    \item \textbf{Unicode Standard}. \url{https://unicode.org/standard/standard.html}
    
    \item \textbf{UTF-8 Everywhere}. \url{http://utf8everywhere.org/}
    
    \item \textbf{C++ Core Guidelines}. \url{https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines}
\end{enumerate}

\newpage

\appendix

\section{Примеры кода}

\subsection{Основной цикл краулера для Wikipedia}

\begin{lstlisting}[language=Python, caption=Краулер для Wikipedia (Python)]
class WikipediaCrawler(BaseCrawler):
    def __init__(self, config: CrawlerConfig):
        super().__init__(config)
        self.api_url = "https://ru.wikipedia.org/w/api.php"
        self.session = requests.Session()
        
    def crawl_article(self, title: str) -> Optional[Document]:
        """Сбор одной статьи Wikipedia"""
        params = {
            "action": "query",
            "format": "json",
            "titles": title,
            "prop": "extracts|info",
            "explaintext": True,
            "inprop": "url"
        }
        
        try:
            response = self.session.get(self.api_url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            pages = data.get("query", {}).get("pages", {})
            for page_id, page_data in pages.items():
                if page_id != "-1":  # Страница существует
                    return Document(
                        url=page_data.get("fullurl", ""),
                        title=page_data.get("title", ""),
                        content=page_data.get("extract", ""),
                        source="wikipedia",
                        timestamp=datetime.now()
                    )
        except Exception as e:
            self.logger.error(f"Error crawling Wikipedia article {title}: {e}")
            
        return None
\end{lstlisting}

\subsection{Стемминг с кешированием}

\begin{lstlisting}[language=C++, caption=Стеммер с кешированием результатов (C++)]
class CachedStemmer : public Stemmer {
private:
    ds::HashTable<ds::String, ds::String> cache_;
    std::shared_mutex cache_mutex_;
    
public:
    ds::String stem(const ds::String& word) override {
        // Проверка кеша (чтение)
        {
            std::shared_lock lock(cache_mutex_);
            auto it = cache_.find(word);
            if (it != cache_.end()) {
                return it->second;
            }
        }
        
        // Выполнение стемминга
        ds::String result = RussianStemmer::stem(word);
        
        // Сохранение в кеш (запись)
        {
            std::unique_lock lock(cache_mutex_);
            cache_[word] = result;
        }
        
        return result;
    }
    
    void clear_cache() {
        std::unique_lock lock(cache_mutex_);
        cache_.clear();
    }
    
    size_t cache_size() const {
        std::shared_lock lock(cache_mutex_);
        return cache_.size();
    }
};
\end{lstlisting}

\subsection{Построение индекса с многопоточностью}

\begin{lstlisting}[language=C++, caption=Индексация документов с использованием пула потоков]
class ParallelIndexBuilder {
private:
    InvertedIndex& index_;
    ThreadPool pool_;
    std::atomic<uint32_t> processed_docs_{0};
    
public:
    void build_index(const std::vector<Document>& documents) {
        const size_t batch_size = 100;
        
        for (size_t i = 0; i < documents.size(); i += batch_size) {
            auto batch_begin = documents.begin() + i;
            auto batch_end = documents.begin() + 
                           std::min(i + batch_size, documents.size());
            
            pool_.enqueue([this, batch_begin, batch_end]() {
                this->process_batch(batch_begin, batch_end);
            });
        }
        
        pool_.wait_all();
    }
    
private:
    void process_batch(DocumentIter begin, DocumentIter end) {
        LocalIndex local_index;
        
        for (auto it = begin; it != end; ++it) {
            uint32_t doc_id = processed_docs_++;
            index_document(*it, doc_id, local_index);
        }
        
        // Слияние локального индекса с основным
        merge_index(local_index);
    }
};
\end{lstlisting}

\subsection{Парсинг запросов с обработкой ошибок}

\begin{lstlisting}[language=C++, caption=Парсер запросов с диагностикой ошибок]
class QueryParser {
public:
    struct ParseResult {
        QueryNode* ast = nullptr;
        std::vector<std::string> errors;
        std::vector<std::string> warnings;
        bool success = false;
    };
    
    ParseResult parse(const std::string& query) {
        ParseResult result;
        
        try {
            auto tokens = tokenize_query(query);
            validate_tokens(tokens, result);
            
            if (!result.errors.empty()) {
                return result;
            }
            
            size_t pos = 0;
            result.ast = parse_expression(pos, tokens);
            result.success = (result.ast != nullptr);
            
            if (pos != tokens.size()) {
                result.errors.push_back("Unexpected tokens at end of query");
                result.success = false;
            }
        } catch (const ParseException& e) {
            result.errors.push_back(e.what());
            result.success = false;
        } catch (const std::exception& e) {
            result.errors.push_back(std::string("Internal error: ") + e.what());
            result.success = false;
        }
        
        return result;
    }
};
\end{lstlisting}

\section{Примеры запросов и результатов}

\begin{table}[H]
\centering
\caption{Дополнительные примеры запросов с фильтрацией по источникам}
\begin{tabular}{p{8cm}r}
\toprule
\textbf{Запрос} & \textbf{Результатов} \\
\midrule
\texttt{алгоритм} & 3,241 \\
\texttt{SOURCE:wikipedia \&\& алгоритм} & 1,845 \\
\texttt{SOURCE:habr \&\& алгоритм} & 967 \\
\texttt{SOURCE:lenta \&\& алгоритм} & 429 \\
\texttt{машинное \&\& обучение} & 1,247 \\
\texttt{SOURCE:wikipedia \&\& "машинное обучение"} & 892 \\
\texttt{python || java || javascript} & 2,891 \\
\texttt{SOURCE:habr \&\& (python || java)} & 1,234 \\
\texttt{(нейронные \&\& сети) || (глубокое \&\& обучение)} & 1,523 \\
\texttt{SOURCE:wikipedia \&\& "нейронные сети"} & 634 \\
\texttt{SOURCE:habr \&\& "глубокое обучение"} & 345 \\
\texttt{программирование \&\& !java} & 1,782 \\
\texttt{"искусственный интеллект"} & 523 \\
\texttt{SOURCE:lenta \&\& "искусственный интеллект"} & 187 \\
\texttt{"машинное обучение" / 5} & 1,634 \\
\texttt{(компьютер || вычисление) \&\& (наука || технология)} & 4,123 \\
\texttt{SOURCE:wikipedia \&\& компьютер \&\& наука} & 1,845 \\
\texttt{SOURCE:habr \&\& программирование \&\& 2023} & 678 \\
\bottomrule
\end{tabular}
\label{tab:additional_queries}
\end{table}

\end{document}